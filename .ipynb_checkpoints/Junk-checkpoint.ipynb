{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 0
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 4 NLP ML Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image Description](Image/Twitter1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Flow Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda activate TFgpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pip cache purge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.10.0\n",
      "Result: 7.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow version\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Create a simple TensorFlow computation graph\n",
    "a = tf.constant(3.0)\n",
    "b = tf.constant(4.0)\n",
    "c = tf.add(a, b)\n",
    "\n",
    "# Execute the computation graph\n",
    "result = c.numpy()\n",
    "\n",
    "# Print the result\n",
    "print(\"Result:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Python Packages & DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import regexp_tokenize, word_tokenize, RegexpTokenizer, sent_tokenize\n",
    "import string\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from afinn import Afinn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import unicodedata\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "df = pd.read_csv('judge-1377884607_tweet_product_company.csv', encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove tweets with unknown sentiment\n",
    "df = df[df['is_there_an_emotion_directed_at_a_brand_or_product'] != \"I can't tell\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Find better ways to analyze the cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Clean and preprocess the text\n",
    "# def clean_text(text):\n",
    "#     if pd.isnull(text):\n",
    "#         return ''\n",
    "#     else:\n",
    "#         # Convert to lowercase\n",
    "#         text = text.lower()\n",
    "\n",
    "#         # Remove mentions and URLs\n",
    "#         text = re.sub(r'@\\w+', 'USER', text)\n",
    "#         text = re.sub(r'http\\S+|www\\S+', 'URL', text)\n",
    "\n",
    "#         # Remove punctuation\n",
    "#         text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "#         # Tokenization\n",
    "#         tokens = nltk.word_tokenize(text)\n",
    "\n",
    "#         # Remove stopwords\n",
    "#         stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "#         tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "#         # Lemmatization\n",
    "#         lemmatizer = WordNetLemmatizer()\n",
    "#         tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "#         return ' '.join(tokens)\n",
    "\n",
    "# # Apply preprocessing steps to the 'tweet_text' column\n",
    "# df['cleaned_tweet'] = df['tweet_text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and preprocess the text\n",
    "def clean_text(text):\n",
    "    if pd.isnull(text):\n",
    "        return ''\n",
    "    else:\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "\n",
    "        # Remove non-ASCII characters\n",
    "        text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')\n",
    "\n",
    "        # Remove mentions and URLs\n",
    "        text = re.sub(r'@\\w+', 'USER', text)\n",
    "        text = re.sub(r'http\\S+|www\\S+', 'URL', text)\n",
    "\n",
    "        # Remove special characters and symbols\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "        # Tokenization\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "\n",
    "        # Remove stopwords\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "        # Lemmatization\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing steps to the 'tweet_text' column\n",
    "df['cleaned_tweet'] = df['tweet_text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       USER 3g iphone 3 hr tweeting rise_austin dead ...\n",
       "1       USER know USER awesome ipadiphone app youll li...\n",
       "2                         USER wait ipad 2 also sale sxsw\n",
       "3       USER hope year festival isnt crashy year iphon...\n",
       "4       USER great stuff fri sxsw marissa mayer google...\n",
       "                              ...                        \n",
       "9088                            ipad everywhere sxsw link\n",
       "9089    wave buzz rt USER interrupt regularly schedule...\n",
       "9090    google zeiger physician never reported potenti...\n",
       "9091    verizon iphone customer complained time fell b...\n",
       "9092    iiau_eioaaa_a_uart USER google test uicheckin ...\n",
       "Name: cleaned_tweet, Length: 8937, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cleaned_tweet']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into X and y\n",
    "X = df['cleaned_tweet']\n",
    "y = df['is_there_an_emotion_directed_at_a_brand_or_product']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split X and y into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Vectorize the tweets using CountVectorizer\n",
    "# vectorizer = CountVectorizer()\n",
    "# X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "# X_test_vectorized = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the tweets using CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE to the training data\n",
    "smote = SMOTE()\n",
    "X_train_vectorized, y_train = smote.fit_resample(X_train_vectorized, y_train_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get the feature names from the vocabulary dictionary\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert the sparse matrix to a DataFrame for visualization\n",
    "X_vec_df = pd.DataFrame(X_train_vectorized.toarray(), columns=feature_names)\n",
    "print(X_vec_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Issues from Vectorizing\n",
    "\n",
    "1) Remove non-ASCII characters: You can remove non-ASCII characters using Unicode encoding, which can help eliminate any unusual characters that might be causing issues. You can use the unicodedata module or regular expressions to accomplish this.\n",
    "\n",
    "2) Remove special characters and symbols: Remove any special characters, symbols, or emojis that are not relevant to your analysis. You can use regular expressions or specific character replacement techniques for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log Ression Model with Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the input features\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "X_train_scaled = scaler.fit_transform(X_train_vectorized)\n",
    "X_test_scaled = scaler.transform(X_test_vectorized)\n",
    "\n",
    "# Train the logistic regression model on the scaled data\n",
    "logreg = LogisticRegression(max_iter=3000)\n",
    "logreg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the scaled test data\n",
    "y_pred = logreg.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy:', accuracy)\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred))\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Create a Naive Bayes classifier\n",
    "nb = MultinomialNB()\n",
    "\n",
    "# Train the Naive Bayes model\n",
    "nb.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_nb = nb.predict(X_test_vectorized)\n",
    "\n",
    "# Evaluate the Naive Bayes model\n",
    "accuracy_nb = accuracy_score(y_test, y_pred_nb)\n",
    "print('Accuracy (Naive Bayes):', accuracy_nb)\n",
    "print('Classification Report (Naive Bayes):')\n",
    "print(classification_report(y_test, y_pred_nb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create a Random Forest classifier\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Train the Random Forest model\n",
    "rf.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_rf = rf.predict(X_test_vectorized)\n",
    "\n",
    "# Evaluate the Random Forest model\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print('Accuracy (Random Forest):', accuracy_rf)\n",
    "print('Classification Report (Random Forest):')\n",
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Preprocess the text data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['cleaned_tweet'])\n",
    "X = tokenizer.texts_to_sequences(df['cleaned_tweet'])\n",
    "X = pad_sequences(X)\n",
    "\n",
    "# Convert target labels to categorical\n",
    "y = pd.get_dummies(df['is_there_an_emotion_directed_at_a_brand_or_product']).values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model architecture\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=X.shape[1]))\n",
    "model.add(LSTM(units=64))\n",
    "model.add(Dense(units=y.shape[1], activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_dl_probs = model.predict(X_test)\n",
    "y_pred_dl = np.argmax(y_pred_dl_probs, axis=1)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_dl = accuracy_score(np.argmax(y_test, axis=1), y_pred_dl)\n",
    "print('Accuracy (Deep Learning):', accuracy_dl)\n",
    "print('Classification Report (Deep Learning):')\n",
    "print(classification_report(np.argmax(y_test, axis=1), y_pred_dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Create a dummy classifier that predicts the majority class\n",
    "dummy = DummyClassifier(strategy='most_frequent')\n",
    "\n",
    "# Train the dummy classifier\n",
    "dummy.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions using the dummy classifier\n",
    "y_pred_dummy = dummy.predict(X_test)\n",
    "\n",
    "# Evaluate the dummy model\n",
    "accuracy_dummy = accuracy_score(y_test, y_pred_dummy)\n",
    "print('Accuracy (Dummy Model):', accuracy_dummy)\n",
    "print('Classification Report (Dummy Model):')\n",
    "print(classification_report(y_test, y_pred_dummy, zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review Top Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Tokenize each cleaned tweet\n",
    "tokenized_tweets = [word_tokenize(tweet) for tweet in df['cleaned_tweet']]\n",
    "\n",
    "# Flatten the list of tokenized tweets\n",
    "all_tokens = [token for tweet_tokens in tokenized_tweets for token in tweet_tokens]\n",
    "\n",
    "# Count the occurrences of each token\n",
    "token_counts = nltk.FreqDist(all_tokens)\n",
    "\n",
    "# Get the top tokens and their counts\n",
    "num_top_tokens = 50  # Choose the desired number of top tokens to display\n",
    "top_tokens = token_counts.most_common(num_top_tokens)\n",
    "\n",
    "# Extract the tokens and counts for plotting\n",
    "tokens = [token for token, count in top_tokens]\n",
    "counts = [count for token, count in top_tokens]\n",
    "\n",
    "# Create the bar graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(tokens, counts)\n",
    "plt.xlabel('Tokens')\n",
    "plt.ylabel('Counts')\n",
    "plt.title('Top Tokens')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine preprocessed text into a single string\n",
    "all_text = ' '.join(df['cleaned_tweet'])\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([all_text])\n",
    "\n",
    "# Get the word-to-index mapping\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "total_tokens = len(word_index)\n",
    "print(\"Total unique tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "# Get the token counts\n",
    "token_counts = FreqDist(all_text.split())\n",
    "\n",
    "# Print the most common tokens and their frequencies\n",
    "num_common_tokens = 10  # Choose the desired number of most common tokens to display\n",
    "common_tokens = token_counts.most_common(num_common_tokens)\n",
    "for token, frequency in common_tokens:\n",
    "    print(\"Token:\", token, \"\\tFrequency:\", frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understand Vectorizor \n",
    "- Does this replace needing to clean the data (lowercase, ect)? Where does it fit in the overall model process?\n",
    "\n",
    "# Step 1: Feature Engineering\n",
    "- Before training a sentiment analysis model, it's important to engineer relevant features from your data. Some possible features you can consider are:\n",
    "\n",
    "- Word frequency: Calculate the frequency of each word in the tweet text and add it as a feature.\n",
    "- Character count: Count the number of characters in each tweet and add it as a feature.\n",
    "- Hashtags and mentions: Extract hashtags and mentions from the tweets and add them as features.\n",
    "- Sentiment lexicons: Utilize pre-built sentiment lexicons such as AFINN or SentiWordNet and assign sentiment scores to words in the tweets.\n",
    "\n",
    "# Word Frequency:\n",
    "\n",
    " 1) Calculate the frequency of each word in the tweet text: Count the occurrences of each word in the tweet and create a feature that represents the frequency of each word. Tools: You can use the Counter class from the Python collections module to count word frequencies.\n",
    "\n",
    "2) Character Count: Count the number of characters in each tweet: Determine the length of each tweet in terms of the number of characters and add it as a feature. Tools: You can use the len() function in Python to calculate the length of each tweet.\n",
    "\n",
    "3) Hashtags and Mentions: Extract hashtags and mentions from the tweets: Parse the tweet text to identify hashtags (words or phrases preceded by the '#' symbol) and mentions (usernames preceded by the '@' symbol). Add them as features: Create binary features indicating the presence or absence of specific hashtags or mentions in the tweet. Tools: You can use regular expressions (re module) in Python to extract hashtags and mentions from the tweet text.\n",
    "\n",
    "4) Sentiment Lexicons: Utilize pre-built sentiment lexicons: Sentiment lexicons are dictionaries or databases that associate words with sentiment scores. They can help assign sentiment scores to individual words in the tweets. Assign sentiment scores: Look up each word in the tweet text in the sentiment lexicon and assign sentiment scores to those words. Tools: You can use pre-built sentiment lexicons such as AFINN (which assigns a sentiment score between -5 and +5 to words) or SentiWordNet (which provides sentiment scores based on WordNet synsets).\n",
    "\n",
    "# Dummy Model\n",
    "- Build a dummy model\n",
    "\n",
    "# Step 2: Splitting the Data\n",
    "- Split your dataset into training and testing sets. This will allow you to train your model on a portion of the data and evaluate its performance on unseen data. The commonly used split ratio is 80% for training and 20% for testing. You can use the train_test_split function from scikit-learn to perform the split.\n",
    "\n",
    "# Step 3: Choosing a Model\n",
    "\n",
    "- Select an appropriate machine learning model for sentiment analysis. Some popular choices are:\n",
    "\n",
    "# Naive Bayes Classifier: It is simple and effective for text classification tasks.\n",
    "- Support Vector Machines (SVM): It performs well in high-dimensional spaces and is often used for text classification.\n",
    "- Recurrent Neural Networks (RNN) or Long Short-Term Memory (LSTM) networks: These models can capture the sequential nature of text data.\n",
    "\n",
    "# Step 4: Model Training and Evaluation\n",
    "- Train your selected model on the training set using the engineered features. Evaluate the performance of the trained model on the testing set. Common evaluation metrics for sentiment analysis include accuracy, precision, recall, and F1-score. You can use scikit-learn's fit() and predict() functions to train and evaluate the model.\n",
    "\n",
    "# Step 5: Hyperparameter Tuning\n",
    "- Optimize the performance of your model by tuning its hyperparameters. This involves trying different combinations of hyperparameters and selecting the ones that result in the best performance. You can use techniques like grid search or random search to find the optimal hyperparameters.\n",
    "\n",
    "# Step 6: Model Deployment\n",
    "- Once you have trained and fine-tuned your sentiment analysis model, you can deploy it to make predictions on new, unseen data. This can involve integrating the model into a web application, creating an API, or using it for real-time sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfgpu",
   "language": "python",
   "name": "tfgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
