{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 0
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 4 NLP ML Project\n",
    "\n",
    "- What trends are we seeing. What do people like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image Description](Image/Twitter1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Flow Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda activate TFgpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pip cache purge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.10.0\n",
      "Result: 7.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow version\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Create a simple TensorFlow computation graph\n",
    "a = tf.constant(3.0)\n",
    "b = tf.constant(4.0)\n",
    "c = tf.add(a, b)\n",
    "\n",
    "# Execute the computation graph\n",
    "result = c.numpy()\n",
    "\n",
    "# Print the result\n",
    "print(\"Result:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Python Packages & DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import regexp_tokenize, word_tokenize, RegexpTokenizer, sent_tokenize\n",
    "import string\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from afinn import Afinn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import unicodedata\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whole Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tweet tokenizer NLTK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        if isinstance(text, str):  # Check if text is a non-null string\n",
    "            # Remove punctuation and convert to lowercase\n",
    "            text = ''.join([char.lower() for char in text if char.isalnum() or char.isspace()])\n",
    "            # Remove stop words\n",
    "            text = ' '.join([word for word in text.split() if word not in self.stop_words])\n",
    "        else:\n",
    "            text = ''  # Replace NaN values with an empty string\n",
    "        return text\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return [self.preprocess_text(text) for text in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "df = pd.read_csv('judge-1377884607_tweet_product_company.csv', encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove tweets with unknown sentiment\n",
    "df = df[df['is_there_an_emotion_directed_at_a_brand_or_product'] != \"I can't tell\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the data\n",
    "X = df['tweet_text']\n",
    "y = df['is_there_an_emotion_directed_at_a_brand_or_product']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5953820780648708\n"
     ]
    }
   ],
   "source": [
    "# Create the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', TextPreprocessor()),  # Custom text preprocessing\n",
    "    ('tfidf', TfidfVectorizer()),  # TF-IDF feature extraction\n",
    "    ('classifier', DummyClassifier(strategy='most_frequent'))  # Dummy Classifier\n",
    "])\n",
    "\n",
    "# Train the model and make predictions\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6812080536912751\n"
     ]
    }
   ],
   "source": [
    "# Create the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', TextPreprocessor()),  # Custom text preprocessing\n",
    "    ('tfidf', TfidfVectorizer()),  # TF-IDF feature extraction\n",
    "    ('classifier', LinearSVC())  # Linear Support Vector Classifier (SVC)\n",
    "])\n",
    "\n",
    "# Train the model and make predictions\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6649888143176734\n"
     ]
    }
   ],
   "source": [
    "# Create the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', TextPreprocessor()),  # Custom text preprocessing\n",
    "    ('tfidf', TfidfVectorizer()),  # TF-IDF feature extraction\n",
    "    ('classifier', MultinomialNB())  # Naive Bayes classifier\n",
    "])\n",
    "\n",
    "# Train the model and make predictions\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6580538757559098\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode the target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', TextPreprocessor()),  # Custom text preprocessing\n",
    "    ('tfidf', TfidfVectorizer()),  # TF-IDF feature extraction\n",
    "    ('classifier', XGBClassifier())  # XGBoost classifier\n",
    "])\n",
    "\n",
    "# Train the model and make predictions\n",
    "pipeline.fit(X_train, y_train_encoded)\n",
    "y_pred_encoded = pipeline.predict(X_test)\n",
    "\n",
    "# Decode the predictions\n",
    "y_pred = label_encoder.inverse_transform(y_pred_encoded)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XG Boost Dec Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy: 0.6655194581129436\n",
      "Test Accuracy: 0.6580538757559098\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Encode the target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', TextPreprocessor()),  # Custom text preprocessing\n",
    "    ('tfidf', TfidfVectorizer()),  # TF-IDF feature extraction\n",
    "    ('classifier', XGBClassifier())  # XGBoost classifier\n",
    "])\n",
    "\n",
    "# Define the hyperparameters to search\n",
    "hyperparameters = {\n",
    "    'classifier__n_estimators': [100, 200, 300],  # Number of trees in the forest\n",
    "    'classifier__learning_rate': [0.1, 0.01, 0.001],  # Learning rate for each tree\n",
    "    'classifier__max_depth': [3, 5, 7]  # Maximum depth of each tree\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(pipeline, hyperparameters, cv=5)\n",
    "grid_search.fit(X_train, y_train_encoded)\n",
    "\n",
    "# Get the best model and its accuracy\n",
    "best_model = grid_search.best_estimator_\n",
    "best_accuracy = grid_search.best_score_\n",
    "print(f'Best Accuracy: {best_accuracy}')\n",
    "\n",
    "# Make predictions using the best model\n",
    "y_pred_encoded = best_model.predict(X_test)\n",
    "\n",
    "# Decode the predictions\n",
    "y_pred = label_encoder.inverse_transform(y_pred_encoded)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Test Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy: 0.6586483075483203\n",
      "Test Accuracy: 0.6690489279824079\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', TextPreprocessor()),  # Custom text preprocessing\n",
    "    ('tfidf', TfidfVectorizer()),  # TF-IDF feature extraction\n",
    "    ('classifier', MultinomialNB())  # Naive Bayes classifier\n",
    "])\n",
    "\n",
    "# Define the hyperparameters to search\n",
    "hyperparameters = {\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2)],  # N-gram range for TF-IDF\n",
    "    'classifier__alpha': [0.1, 1.0, 10.0]  # Smoothing parameter for Naive Bayes\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(pipeline, hyperparameters, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model and its accuracy\n",
    "best_model = grid_search.best_estimator_\n",
    "best_accuracy = grid_search.best_score_\n",
    "print(f'Best Accuracy: {best_accuracy}')\n",
    "\n",
    "# Make predictions using the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Test Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6812080536912751\n"
     ]
    }
   ],
   "source": [
    "# Create the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', TextPreprocessor()),  # Custom text preprocessing\n",
    "    ('tfidf', TfidfVectorizer()),  # TF-IDF feature extraction\n",
    "    ('classifier', RandomForestClassifier())  # Random Forest classifier\n",
    "])\n",
    "\n",
    "# Train the model and make predictions\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "228/228 [==============================] - 13s 51ms/step - loss: 0.0233 - accuracy: 0.9978 - val_loss: 1.6334e-05 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "228/228 [==============================] - 11s 50ms/step - loss: 1.2688e-05 - accuracy: 1.0000 - val_loss: 9.1505e-06 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "228/228 [==============================] - 11s 50ms/step - loss: 7.8489e-06 - accuracy: 1.0000 - val_loss: 6.2270e-06 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "228/228 [==============================] - 11s 49ms/step - loss: 5.5835e-06 - accuracy: 1.0000 - val_loss: 4.6249e-06 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "228/228 [==============================] - 11s 50ms/step - loss: 4.2518e-06 - accuracy: 1.0000 - val_loss: 3.6084e-06 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "228/228 [==============================] - 11s 49ms/step - loss: 3.3726e-06 - accuracy: 1.0000 - val_loss: 2.9037e-06 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "228/228 [==============================] - 11s 49ms/step - loss: 2.7320e-06 - accuracy: 1.0000 - val_loss: 2.3907e-06 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "228/228 [==============================] - 11s 50ms/step - loss: 2.2704e-06 - accuracy: 1.0000 - val_loss: 1.9991e-06 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "228/228 [==============================] - 11s 49ms/step - loss: 1.9030e-06 - accuracy: 1.0000 - val_loss: 1.6927e-06 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "228/228 [==============================] - 11s 50ms/step - loss: 1.6209e-06 - accuracy: 1.0000 - val_loss: 1.4467e-06 - val_accuracy: 1.0000\n",
      "57/57 [==============================] - 1s 13ms/step\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "class TextPreprocessor(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        if isinstance(text, str):  # Check if text is a non-null string\n",
    "            # Remove punctuation and convert to lowercase\n",
    "            text = ''.join([char.lower() for char in text if char.isalnum() or char.isspace()])\n",
    "            # Remove stop words\n",
    "            text = ' '.join([word for word in text.split() if word not in self.stop_words])\n",
    "        else:\n",
    "            text = ''  # Replace NaN values with an empty string\n",
    "        return text\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return [self.preprocess_text(text) for text in X]\n",
    "    \n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X)\n",
    "\n",
    "# Load and preprocess the data\n",
    "df = pd.read_csv('judge-1377884607_tweet_product_company.csv', encoding='latin1')\n",
    "X = df['tweet_text']\n",
    "y = df['is_there_an_emotion_directed_at_a_brand_or_product']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', TextPreprocessor()),  # Custom text preprocessing\n",
    "    ('tfidf', TfidfVectorizer()),  # TF-IDF feature extraction\n",
    "])\n",
    "\n",
    "# Preprocess the text data\n",
    "X_train_processed = pipeline.named_steps['preprocessor'].transform(X_train)\n",
    "X_test_processed = pipeline.named_steps['preprocessor'].transform(X_test)\n",
    "\n",
    "# Convert the processed text data to a list of strings\n",
    "X_train_processed = [str(text) for text in X_train_processed]\n",
    "X_test_processed = [str(text) for text in X_test_processed]\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train_processed)\n",
    "\n",
    "X_train_tokenized = tokenizer.texts_to_sequences(X_train_processed)\n",
    "X_test_tokenized = tokenizer.texts_to_sequences(X_test_processed)\n",
    "\n",
    "# Pad sequences\n",
    "max_sequence_length = max(max(len(x) for x in X_train_tokenized), max(len(x) for x in X_test_tokenized))\n",
    "X_train_padded = pad_sequences(X_train_tokenized, maxlen=max_sequence_length)\n",
    "X_test_padded = pad_sequences(X_test_tokenized, maxlen=max_sequence_length)\n",
    "\n",
    "# Create the deep learning model\n",
    "embedding_dim = 100\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(tokenizer.word_index) + 1, embedding_dim, input_length=max_sequence_length))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Convert target variable to binary values\n",
    "y_train_binary = (y_train == 'positive').astype(int)\n",
    "y_test_binary = (y_test == 'positive').astype(int)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_padded, y_train_binary, validation_data=(X_test_padded, y_test_binary), epochs=10, batch_size=32)\n",
    "\n",
    "# Predict probabilities\n",
    "y_pred_prob = model.predict(X_test_padded)\n",
    "\n",
    "# Convert probabilities to class labels\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test_binary, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and preprocess the text\n",
    "def clean_text(text, custom_stopwords=None):\n",
    "    if pd.isnull(text):\n",
    "        return ''\n",
    "    else:\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "\n",
    "        # Remove non-ASCII characters\n",
    "        text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')\n",
    "\n",
    "        # Remove mentions and URLs\n",
    "        text = re.sub(r'@\\w+', 'USER', text)\n",
    "        text = re.sub(r'http\\S+|www\\S+', 'URL', text)\n",
    "\n",
    "        # Remove special characters and symbols\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "        # Tokenization\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "\n",
    "        # Remove stopwords\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        if custom_stopwords:\n",
    "            stop_words.update(custom_stopwords)\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "        # Lemmatization\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "        return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfgpu",
   "language": "python",
   "name": "tfgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
